{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing Using Keras and Tensorflow - Chris Kim\n",
    "\n",
    "This notebook was adapted from github user [nirmalyaghosh](https://github.com/nirmalyaghosh/deep-learning-vm), and user [fchollet](https://github.com/fchollet/keras/issues/3109). Lastly, most information was adapted from a great website called [Machine Learning Mastery](http://machinelearningmastery.com/).\n",
    "\n",
    "The code below is my first attempt to use image processing using the tools made available in Python, namely `keras` and `TensorFlow` as the backend. This exploration was inspired by [doyleax](https://doyleax.github.io/Portfolio/capstone.html) and her Capstone project. Side profile images of 1780 pairs of shoes are transformed to be evaluated in a neural network to try to predict whether a shoe is classified as rare or not (determined by a metric composed of retail price and average deadstock price).\n",
    "\n",
    "All images are taken from [StockX](www.stockx.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(7)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('shoenumbers.pickle', 'rb') as handle:\n",
    "    X = pickle.load(handle, encoding='bytes')\n",
    "with open('targetvars.pickle', 'rb') as handle:\n",
    "    y = pickle.load(handle, encoding='bytes')\n",
    "\n",
    "print(np.array(X).shape)\n",
    "print(np.array(y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.502247191011\n"
     ]
    }
   ],
   "source": [
    "baseline = y.mean()\n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1780, 100, 140, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change size of images so it's not so huge when we pass it through keras\n",
    "import cv2\n",
    "new_X = []\n",
    "for row in range(len(X)):\n",
    "    new_X.append(cv2.resize(X[row], (140,100)))\n",
    "np.array(new_X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare data for training and testing\n",
    "X = np.array(new_X)\n",
    "X = X.astype('float32')\n",
    "X /= 255.0\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model creation by machinelearningmastery\n",
    "num_classes=2\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(100,140,3), activation='relu', padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "epochs = 25\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1335 samples, validate on 445 samples\n",
      "Epoch 1/25\n",
      "1335/1335 [==============================] - 103s - loss: 7.8471 - acc: 0.5019 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 2/25\n",
      "1335/1335 [==============================] - 111s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 3/25\n",
      "1335/1335 [==============================] - 116s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 4/25\n",
      "1335/1335 [==============================] - 109s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 5/25\n",
      "1335/1335 [==============================] - 107s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 6/25\n",
      "1335/1335 [==============================] - 110s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 7/25\n",
      "1335/1335 [==============================] - 111s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 8/25\n",
      "1335/1335 [==============================] - 113s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 9/25\n",
      "1335/1335 [==============================] - 108s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 10/25\n",
      "1335/1335 [==============================] - 102s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 11/25\n",
      "1335/1335 [==============================] - 103s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 12/25\n",
      "1335/1335 [==============================] - 102s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 13/25\n",
      "1335/1335 [==============================] - 101s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 14/25\n",
      "1335/1335 [==============================] - 102s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 15/25\n",
      "1335/1335 [==============================] - 102s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 16/25\n",
      "1335/1335 [==============================] - 102s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 17/25\n",
      "1335/1335 [==============================] - 102s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 18/25\n",
      "1335/1335 [==============================] - 102s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 19/25\n",
      "1335/1335 [==============================] - 101s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 20/25\n",
      "1335/1335 [==============================] - 102s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 21/25\n",
      "1335/1335 [==============================] - 101s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 22/25\n",
      "1335/1335 [==============================] - 104s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 23/25\n",
      "1335/1335 [==============================] - 112s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 24/25\n",
      "1335/1335 [==============================] - 104s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Epoch 25/25\n",
      "1335/1335 [==============================] - 102s - loss: 7.9564 - acc: 0.5064 - val_loss: 8.5118 - val_acc: 0.4719\n",
      "Accuracy: 47.19%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445/445 [==============================] - 6s     \n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Neural Networking did not increase the accuracy of my model of baseline: 50%. In fact, after observing more closely, the model predicted all test data as one category. There are several factors that could have caused this preference.\n",
    "- No image transformations\n",
    "- Not enough epochs\n",
    "- Incorrect parameters/layers used\n",
    "- Not enough data\n",
    "- All of the above\n",
    "\n",
    "I believe that it is most likely all of the above, but most importantly, not enough data. Even Kaggle's dogs vs cats competition contained 25,000 images of dogs and cats, about 14x more images than this dataset. That doesn't even include all the pixels in each picture nor all the image transformations that are possible. But this was definitely a huge leap into the fast-growing field of deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
